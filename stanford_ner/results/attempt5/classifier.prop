# Flags: https://github.com/dat/stanford-ner/blob/ca65e85db0007659c8b5348b076b119230a8405e/src/edu/stanford/nlp/sequences/SeqClassifierFlags.java

############ BASIC SETUP ##################
# Path of file to use as training data
trainFile = ..\\stanford_ner\\silver_tsv\\training_data
# Path to serialize classifier to
serializeTo = ner-model.ser.gz
# On tab-separated column data, it says what is in each column
# This applies at training time or if testing on tab-separated column data. It says what is in each column. It doesn't apply when running on plain text data. The simplest scenario for training is having words and classes in two column. word=0,answer=1 is the default if conllNoTags is specified; otherwise word=0,tag=1,answer=2 is the default. But you can add other columns, such as for a part-of-speech tag, presences in a lexicon, etc. That would only be useful at runtime if you have part-of-speech information or whatever available and are passing it in with the tokens (that is, you can pass to classify CoreLabel tokens with additional fields stored in them).
# https://github.com/stanfordnlp/CoreNLP/blob/d558d95d80b36b5b45bc21882cbc0ef7452eda24/src/edu/stanford/nlp/ling/AnnotationLookup.java
map = word=0,answer=1

############ SAVING MEMORY ##################
# quasi-Newton optimizer (L-BFGS) - Number of past guesses
qnSize=5
# feature names aren't actually needed while the core model estimation (optimization) code is running
saveFeatureIndexToDisk=true
# This makes it so that you can only label adjacent words with label sequences that were seen next to each other in the training data.
useObservedSequencesOnly=true
# In training, CRFClassifier will train one model, drop all the features with weight (absolute value) beneath the given threshold, and then train a second model.
#featureDiffThresh=0.05
# Convergence tolerance in optimization
tolerance=1e-4
# If true, record the NGram features that correspond to a String (under the current option settings) and reuse rather than recalculating if the String is seen again.
cacheNGrams=false
# Whether or not to split the data into separate documents for training/testing
splitDocuments=true
# If this number is greater than 0, attempt to split documents bigger than this value into multiple documents at sentence boundaries during testing; otherwise do nothing.
maxDocSize=10000

############ ADDITIONAL INFO ###################
# see all the features generated - Options that generate huge numbers of features include useWordPairs and useNGrams when maxNGramLeng is a large number.
# print out all the features generated by the classifier for a dataset to a file based on this name (starting with "features-", suffixed "-1" and "-2" for train and test). This simply prints the feature names, one per line.
printFeatures=list
# Print out features for only the first this many datums, if the value is positive.
printFeaturesUpto=100
# Style in which to print the classifier. One of: HighWeight, HighMagnitude, Collection, AllWeights, WeightHistogram
printClassifier=HighMagnitude
# A parameter to the printing style, which may give, for example the number of parameters to print
printClassifierParam=100
# Print out all feature/class pairs and their weight, and then for each input data point, print justification (weights) for active features
justify=true

############ FORMATTING ################
# If set, convert the labeling of classes (but not the background) into one of several alternate encodings (IO, IOB1, IOB2, IOE1, IOE2, SBIEO, with a S(ingle), B(eginning), E(nding), I(nside) 4-way classification for each class. By default, we either do no re-encoding, or the CoNLLDocumentIteratorFactory does a lossy encoding as IO. Note that this is all CoNLL-specific, and depends on their way of prefix encoding classes, and is only implemented by the CoNLLDocumentIteratorFactory.
#entitySubclassification="SBIEO"
# If true, rather than undoing a recoding of entity tag subtypes (such as BIO variants), just leave them in the output.
#retainEntitySubclassification=false
# Whether to merge B- and I- tags.
#mergeTags=false
# If this is true, some words are normalized: day and month names are lowercased (as for normalizeTimex) and some British spellings are mapped to American English spellings (e.g., -our/-or, etc.).
#normalizeTerms=false
# If this is true, capitalization of day and month names is normalized to lowercase
#normalizeTimex=false
# Blank lines
#deleteBlankLines=false

############ CLASSIFIER PARAMETERS ###############
# the order of the CRF
# The number of things to the left that have to be cached to run the Viterbi algorithm: the maximum context of class features used.
	maxLeft=1
# The number of things to the right that have to be cached to run the Viterbi algorithm: the maximum context of class features used. The maximum possible clique size to use is (maxLeft + maxRight + 1)
		maxRight=1
# As an override to whatever other options are in effect, deletes all features other than C and CpC clique features when building the classifier
#strictlyFirstOrder=false

# If true, any features you include in the map will be incorporated into the model with values equal to those given in the file; values are treated as strings unless you use the "realValued" option (described below)
#useGenericFeatures=false
# For the CMMClassifier (only) if this is true then the Scorer normalizes scores as probabilities.
#normalize=false
# Use a Huber loss prior rather than the default quadratic loss.
#useHuber=false
# Use a Quartic prior rather than the default quadratic loss.
#useQuartic=false
#sigma=1.0
# Used only as a parameter in the Huber loss: this is the distance from 0 at which the loss changes from quadratic to linear
#epsilon=0.01
#beamSize=30

############ OTHERS ############
# Include a feature for the class (as a class marginal). Puts a prior on the classes which is equivalent to how often the feature appeared in the training data.
	useClassFeature=true

# Make features from letter n-grams, i.e., substrings of the word
	useNGrams=false
# Do not include character n-gram features for n-grams that contain neither the beginning nor the end of the word
		noMidNGrams=false
# If this number is positive, n-grams above this size will not be used in the model
		maxNGramLeng=4
# Make features from letter n-grams only lowercase
	lowercaseNGrams=true
# Remove hyphens before making features from letter n-grams <-> tokenizer
#dehyphenateNGrams=true
#Conjoin word shape and n-gram features
	conjoinShapeNGrams=false
# Use letter n-grams for the previous and current words in the CpC clique. This feature helps languages such as Chinese, but not so much for English
	useNeighborNGrams=false

# Include in features giving disjunctions of words anywhere in the left or right disjunctionWidth words (preserving direction but not position)
	useDisjunctive=true
# The number of words on each side of the current word that are included in the disjunction features
	disjunctionWidth=2
# Include in features giving disjunctions of words anywhere in the left or right disjunctionWidth words (preserving direction but not position) interacting with the word shape of the current word
	useDisjunctiveShapeInteraction=false
# Include in features giving disjunctions of words anywhere in the left or right wideDisjunctionWidth words (preserving direction but not position)
	useWideDisjunctive=false
# The number of words on each side of the current word that are included in the disjunction features
	wideDisjunctionWidth=3

# Use combination of initial position in sentence and class (and word shape) as a feature. (Doesn't seem to help.)
	useBeginSent=true
# Include features giving disjunctions of word shapes anywhere in the (left or - not true) right disjunctionWidth words (preserving direction but not position)
# Features: ( thisType-disjType, disjType ) where type comes from wordShape
	useDisjShape=true

# Gives you feature for w
	useWord=true
# Gives you feature for (pw,c), and together with other options enables other previous features, such as (pt,c) [with useTags)
	usePrev=true
# Gives you feature for (nw,c), and together with other options enables other next features, such as (nt,c) [with useTags)
	useNext=true
# Gives you features for (pw, w, c) and (w, nw, c)
	useWordPairs=false

	# Use class combination features
		useSequences=true
	# Use class combination features using previous classes
		usePrevSequences=true
	# Use class combination features using next classes
			useNextSequences=false
	# Use plain higher-order state sequences out to minimum of length or maxLeft
			useLongSequences=false
	# Use extra second order class sequence features when previous is CoNLL boundary, so entity knows it can span boundary.
			useBoundarySequences=false
	# Use first, second, and third order class and tag sequence interaction features
			useTaggySequences=false
	# Add in sequences of tags with just current class features
			useExtraTaggySequences=false
	# Add in terms that join sequences of 2 or 3 tags with the current shape
			useTaggySequencesShapeInteraction=false
	# Don't extend the range of useTaggySequences when maxLeft is increased.
			dontExtendTaggy=false

	# Gives you features (pt, t, nt, c), (t, nt, c), (pt, t, c)
			useSymTags=false
# Gives you features (pw, nw, c)
		useSymWordPairs=false

	# Use basic zeroeth order word shape features.
		useTypeSeqs=true
	# Add additional first and second order word shape features
		useTypeSeqs2=true
	# Adds one more first order shape sequence
			useTypeSeqs3=false

# Some first order word shape patterns.
		useTypeySequences=false

# If the prev word is of length 3 or less, add an extra feature that combines the word two back and the current word's shape. Weird!
	useLastRealWord=false
# If the next word is of length 3 or less, add an extra feature that combines the word after next and the current word's shape. Weird!
	useNextRealWord=true
# Match a word against a list of name titles (Mr, Mrs, etc.). Doesn't really seem to help.
	useTitle=false
# Match a word against a better list of English name titles (Mr, Mrs, etc.). Still doesn't really seem to help.
	useTitle2=false

# This is a very engineered feature designed to capture multiple references to names. If the current word isn't capitalized, followed by a non-capitalized word, and preceded by a word with alphabetic characters, it returns NO-OCCURRENCE-PATTERN. Otherwise, if the previous word is a capitalized NNP, then if in the next 150 words you find this PW-W sequence, you get XY-NEXT-OCCURRENCE-XY, else if you find W you get XY-NEXT-OCCURRENCE-Y. Similarly for backwards and XY-PREV-OCCURRENCE-XY and XY-PREV-OCCURRENCE-Y. Else (if the previous word isn't a capitalized NNP), under analogous rules you get one or more of X-NEXT-OCCURRENCE-YX, X-NEXT-OCCURRENCE-XY, X-NEXT-OCCURRENCE-X, X-PREV-OCCURRENCE-YX, X-PREV-OCCURRENCE-XY, X-PREV-OCCURRENCE-X.
	useOccurrencePatterns=false	

# Either "none" for no wordShape use, or the name of a word shape function recognized by WordShapeClassifier.lookupShaper(String) //chris2useLC
# https://github.com/dat/stanford-ner/blob/master/src/edu/stanford/nlp/process/WordShapeClassifier.java#L285
wordShape=chris1

# If true, use gazette features (defined by other flags)
useGazettes=false
# Include the lemma of a word as a feature.
useLemmas=false
# Include word and tag pair features
useWordTag=false
# Load a file of distributional similarity classes (specified by distSimLexicon) and use it for features
#useDistSim=false
# The file to be loaded for distsim classes.
#distSimLexicon=
# Files should be formatted as tab separated rows where each row is a word/class pair. alexclark=word first, terrykoo=class first
#distSimFileFormat=alexclark

	# If true, (String) intern read in data and classes and feature (pre-)names such as substring features
			intern=false
	# If true, intern all (final) feature names (if only current word and ngram features are used, these will already have been interned by intern, and this is an unnecessary no-op)
			intern2=false

############# USELESS ############
# Use combination of position in sentence and class as a feature
usePosition=false
# Conjoin shape with tag or position
useShapeConjunctions=false

############# NO DESCRIPTION ###############
		useSum=false
		selfTest=false
	useReverse=false
		useNB=false

############# UNUSED ###############
# If non-null, treat as a sequence of comma separated integer bounds, where items above the previous bound up to the next bound are binned Len-range
#useBinnedLength=null

