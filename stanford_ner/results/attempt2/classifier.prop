############ BASIC SETUP ##################
# Path of file to use as training data
trainFile = ..\\stanford_ner\\silver_tsv\\training_data
# Path to serialize classifier to
serializeTo = ner-model.ser.gz
# On tab-separated column data, it says what is in each column
map = word=0,answer=1

############ SAVING MEMORY ##################
# quasi-Newton optimizer (L-BFGS) - Number of past guesses
qnSize=5
# feature names aren't actually needed while the core model estimation (optimization) code is running
saveFeatureIndexToDisk=true
# This makes it so that you can only label adjacent words with label sequences that were seen next to each other in the training data.
useObservedSequencesOnly=true
# In training, CRFClassifier will train one model, drop all the features with weight (absolute value) beneath the given threshold, and then train a second model.
#featureDiffThresh=0.05
# Convergence tolerance in optimization
tolerance=1e-4
# If true, record the NGram features that correspond to a String (under the current option settings) and reuse rather than recalculating if the String is seen again.
cacheNGrams=false
# Whether or not to split the data into separate documents for training/testing
splitDocuments=true
# If this number is greater than 0, attempt to split documents bigger than this value into multiple documents at sentence boundaries during testing; otherwise do nothing.
maxDocSize=10000

############ ADDITIONAL INFO ###################
# see all the features generated - Options that generate huge numbers of features include useWordPairs and useNGrams when maxNGramLeng is a large number.
# print out all the features generated by the classifier for a dataset to a file based on this name (starting with "features-", suffixed "-1" and "-2" for train and test). This simply prints the feature names, one per line.
printFeatures=list
# Print out features for only the first this many datums, if the value is positive.
printFeaturesUpto=100
# Style in which to print the classifier. One of: HighWeight, HighMagnitude, Collection, AllWeights, WeightHistogram
printClassifier=AllWeights
# A parameter to the printing style, which may give, for example the number of parameters to print
printClassifierParam=100
# Print out all feature/class pairs and their weight, and then for each input data point, print justification (weights) for active features
justify=true

############ OPTIONAL FLAGS ################
# Include a feature for the class (as a class marginal). Puts a prior on the classes which is equivalent to how often the feature appeared in the training data.
	useClassFeature=true

# If set, convert the labeling of classes (but not the background) into one of several alternate encodings (IO, IOB1, IOB2, IOE1, IOE2, SBIEO, with a S(ingle), B(eginning), E(nding), I(nside) 4-way classification for each class. By default, we either do no re-encoding, or the CoNLLDocumentIteratorFactory does a lossy encoding as IO. Note that this is all CoNLL-specific, and depends on their way of prefix encoding classes, and is only implemented by the CoNLLDocumentIteratorFactory.
#entitySubclassification="SBIEO"
# If true, rather than undoing a recoding of entity tag subtypes (such as BIO variants), just leave them in the output.
#retainEntitySubclassification=false
# Whether to merge B- and I- tags.
#mergeTags=false

# If this is true, some words are normalized: day and month names are lowercased (as for normalizeTimex) and some British spellings are mapped to American English spellings (e.g., -our/-or, etc.).
	normalizeTerms=false

# Make features from letter n-grams, i.e., substrings of the word
	useNGrams=false
# Do not include character n-gram features for n-grams that contain neither the beginning nor the end of the word
	noMidNGrams=true
# If this number is positive, n-grams above this size will not be used in the model
	maxNGramLeng=6

# Include in features giving disjunctions of words anywhere in the left or right disjunctionWidth words (preserving direction but not position)
	useDisjunctive=false
# The number of words on each side of the current word that are included in the disjunction features
	disjunctionWidth=4

# Use combination of position in sentence and class as a feature
	usePosition=true

# Gives you feature for w
		useWord=true
# Gives you feature for (pw,c), and together with other options enables other previous features, such as (pt,c) [with useTags)
	usePrev=false
# Gives you feature for (nw,c), and together with other options enables other next features, such as (nt,c) [with useTags)
	useNext=true
# Gives you features for (pw, w, c) and (w, nw, c)
	useWordPairs=false

# Use class combination features
	useSequences=true
# Use class combination features using previous classes
	usePrevSequences=true
# Use class combination features using next classes
	useNextSequences=true
# Use plain higher-order state sequences out to minimum of length or maxLeft
		useLongSequences=true
# Use extra second order class sequence features when previous is CoNLL boundary, so entity knows it can span boundary.
		useBoundarySequences=true
# Use first, second, and third order class and tag sequence interaction features
		useTaggySequences=true
# Add in sequences of tags with just current class features
	useExtraTaggySequences=false
# Add in terms that join sequences of 2 or 3 tags with the current shape
	useTaggySequencesShapeInteraction=false
# Don't extend the range of useTaggySequences when maxLeft is increased.
	dontExtendTaggy=false

# the order of the CRF
# The number of things to the left that have to be cached to run the Viterbi algorithm: the maximum context of class features used.
	maxLeft=1
# The number of things to the right that have to be cached to run the Viterbi algorithm: the maximum context of class features used. The maximum possible clique size to use is (maxLeft + maxRight + 1)
		maxRight=1

# Gives you features (pt, t, nt, c), (t, nt, c), (pt, t, c)
		useSymTags=true
# Gives you features (pw, nw, c)
	useSymWordPairs=false

# Use basic zeroeth order word shape features.
	useTypeSeqs=true
# Add additional first and second order word shape features
	useTypeSeqs2=true
# Adds one more first order shape sequence
	useTypeSeqs3=false

# Some first order word shape patterns.
	useTypeySequences=true

# Either "none" for no wordShape use, or the name of a word shape function recognized by WordShapeClassifier.lookupShaper(String)
wordShape=chris2useLC

# If true, use gazette features (defined by other flags)
useGazettes=false
# Include the lemma of a word as a feature.
useLemmas=false
# Include word and tag pair features
useWordTag=false

